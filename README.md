# read_papers
# Comprehensive List of Deep Learning Papers and Concepts with arXiv Links

## Fundamental Concepts and Techniques
1. BatchNorm: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
2. Dropout: [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580)
3. Adam Optimizer: [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

## Computer Vision
### Convolutional Neural Networks (CNNs)
4. CNN Visualization: [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
5. DenseNet: [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
6. VGG: [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
7. ResNet: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
8. EfficientNet: [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
9. MobileNet: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)

### Object Detection
10. Faster R-CNN: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)
11. YOLO: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
12. SSD: [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)

### Image Segmentation
13. U-Net: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)

## Natural Language Processing
### Recurrent Neural Networks
14. LSTM: [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) (Not on arXiv)
15. GRU: [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)

### Transformer-based Models
16. Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
17. BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
18. DeBERTa: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
19. RoBERTa: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
20. GPT-2: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
21. GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

## Generative Models
22. Variational Autoencoder: [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)
23. Diffusion Models: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
24. GAN: [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)

## Reinforcement Learning
25. DQN: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) (Not on arXiv)
26. PPO: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)

## Multi-modal Learning
27. CLIP: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
28. ViT: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

## Graph Neural Networks
29. GCN: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)

## Meta-Learning
30. MAML: [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
